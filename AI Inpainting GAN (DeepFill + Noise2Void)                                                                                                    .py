# -*- coding: utf-8 -*-
"""porposed model ModifiedDeepFillv2_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LDSzRpCdrm1AJJ3QfWjJZwdr51YxeZNy

# **MODIFIED DEEPFILL V2**

Colab code for image inpainting.

## **SETUP**

The below cell does the following-

Clone github repo: https://github.com/mahmoudelharmil/moddeepfillv2.git  
Download the model file
"""

!git clone https://github.com/mahmoudelharmil/moddeepfillv2.git
!gdown "https://drive.google.com/u/0/uc?id=1uMghKl883-9hDLhSiI8lRbHCzCmmRwV-&export=download"
!mv /content/deepfillv2_WGAN_G_epoch40_batchsize4.pth moddeepfillv2/model/deepfillv2_WGAN.pth

!pip install pytorch-fid

"""**Change to the code directory**"""

cd moddeepfillv2

"""## **INPUTS AND MASKS**

The below cell is used to obtain the input images and create/upload masks.  
Please make sure that the right input and mask are correctly given, else the result may not be on the expected lines!

Example image and mask is present in `examples` folder.
"""

# !cd input/
# !mkdir input/img
# !mkdir output/img

#@title Run to upload the input image and generate/upload masks{ display-mode: "form" }
from google.colab import files
from ipywidgets import Button, HBox, VBox, widgets
from IPython.display import display, clear_output
import shutil


from create_mask import create_bbox_mask, create_ff_mask


class StopExecution(Exception):
    def _render_traceback_(self):
        pass


def upload_file():
    uploaded = files.upload()
    try:
        fn = list(uploaded.keys())[0]
    except:
        print ("Please upload a valid image file!")
        raise StopExecution
    print('Uploaded file "{name}" of {length} bytes'.format(name=fn,length=len(uploaded[fn])))
    return fn

    print ("")
    print ("")
    print ("PLEASE RUN THE NEXT CELL")


print ("UPLOAD INPUT FILE")
fn = upload_file()
shutil.move(fn, "./input/img.jpg")

output = widgets.Output()

#@title Draw a Mask, Press Finish, Wait for Inpainting

import base64, os
from IPython.display import HTML, Image
from google.colab.output import eval_js
import cv2
import numpy as np
from base64 import b64decode
import matplotlib.pyplot as plt




def draw(imgm, filename='drawing.png', w=400, h=200, line_width=1):
  display(HTML(canvas_html % (w, h, w,h, filename.split('.')[-1], imgm, line_width)))
  data = eval_js("data")
  binary = b64decode(data.split(',')[1])
  with open(filename, 'wb') as f:
    f.write(binary)

fname = "./input/img.jpg"

image64 = base64.b64encode(open(fname, 'rb').read())
image64 = image64.decode('utf-8')

print(f'Will use {fname} for inpainting')
img = np.array(plt.imread(f'{fname}')[:,:,:3])


canvas_html = """
<style>
.button {
  background-color: #4CAF50;
  border: none;
  color: white;
  padding: 15px 32px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
  font-size: 16px;
  margin: 4px 2px;
  cursor: pointer;
}
</style>
<canvas1 width=%d height=%d>
</canvas1>
<canvas width=%d height=%d>
</canvas>

<button class="button">Finish</button>
<script>
var canvas = document.querySelector('canvas')
var ctx = canvas.getContext('2d')

var canvas1 = document.querySelector('canvas1')
var ctx1 = canvas.getContext('2d')


ctx.strokeStyle = 'red';

var img = new Image();
img.src = "data:image/%s;charset=utf-8;base64,%s";
console.log(img)
img.onload = function() {
  ctx1.drawImage(img, 0, 0);
};
img.crossOrigin = 'Anonymous';

ctx.clearRect(0, 0, canvas.width, canvas.height);

ctx.lineWidth = %d
var button = document.querySelector('button')
var mouse = {x: 0, y: 0}

canvas.addEventListener('mousemove', function(e) {
  mouse.x = e.pageX - this.offsetLeft
  mouse.y = e.pageY - this.offsetTop
})
canvas.onmousedown = ()=>{
  ctx.beginPath()
  ctx.moveTo(mouse.x, mouse.y)
  canvas.addEventListener('mousemove', onPaint)
}
canvas.onmouseup = ()=>{
  canvas.removeEventListener('mousemove', onPaint)
}
var onPaint = ()=>{
  ctx.lineTo(mouse.x, mouse.y)
  ctx.stroke()
}

var data = new Promise(resolve=>{
  button.onclick = ()=>{
    resolve(canvas.toDataURL('image/png'))
  }
})
</script>
"""



draw(image64, filename=f"./input/mask.png", w=img.shape[1], h=img.shape[0], line_width=0.04*img.shape[1])
#@title Show a masked image and save a mask
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (15,5)
plt.rcParams['figure.dpi'] = 200
plt.subplot(131)
with_mask = np.array(plt.imread(f"./input/mask.png")[:,:,:3])
mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)
plt.imshow(mask, cmap='gray')
plt.axis('off')
plt.title('mask')
plt.imsave(f"./input/mask.png",mask, cmap='gray')

plt.subplot(132)
img = np.array(plt.imread(f'{fname}')[:,:,:3])
plt.imshow(img)
plt.axis('off')
plt.title('img')

plt.subplot(133)
img = np.array((1-mask.reshape(mask.shape[0], mask.shape[1], -1))*plt.imread(fname)[:,:,:3])
_=plt.imshow(img)
_=plt.axis('off')
_=plt.title('img * mask')
plt.show()

"""## **INPAINT!!**"""

#@title Continue preprocessing inpainted { display-mode: "form" }


def denoise_colored_image(input_image_path, output_image_path, h=10, templateWindowSize=7, searchWindowSize=21):
    # Read the image
    img = cv2.imread(input_image_path)

    # Separate the image into color channels (BGR format)
    b, g, r = cv2.split(img)

    # Denoise each channel using the Non-Local Means algorithm
    b_denoised = cv2.fastNlMeansDenoising(b, None, h, templateWindowSize, searchWindowSize)
    g_denoised = cv2.fastNlMeansDenoising(g, None, h, templateWindowSize, searchWindowSize)
    r_denoised = cv2.fastNlMeansDenoising(r, None, h, templateWindowSize, searchWindowSize)

    # Merge the denoised channels back together
    denoised_img = cv2.merge([b_denoised, g_denoised, r_denoised])

    # Save the denoised image
    cv2.imwrite(output_image_path, denoised_img)

def sharpen_image(input_image_path,output_image_path):
  img = cv2.imread(input_image_path)
  kernel = np.array([[-1, -1, -1],
                      [-1, 9, -1],
                      [-1, -1, -1]])
  sharpened_img=cv2.filter2D(img, -1, kernel)
  cv2.imwrite(output_image_path, sharpened_img)
  return sharpened_img


# Example usage
input_image_path = 'input/img.jpg'
output_image_path = 'input/imgs/img.jpg'
denoise_colored_image(input_image_path, output_image_path)
#shutil.copyfile(input_image_path, output_image_path)


input_image = cv2.imread("input/img.jpg")
input_image_dn = cv2.imread("input/imgs/img.jpg")

resize_size = (256,256)


# f, axarr = plt.subplots(1,2, figsize=(15,15))
# axarr[0].imshow(cv2.cvtColor(cv2.resize(input_image, resize_size), cv2.COLOR_BGR2RGB))
# axarr[0].title.set_text('Actual Input')
# axarr[0].axis('off')

# axarr[1].imshow(cv2.cvtColor(cv2.resize(input_image_dn, resize_size), cv2.COLOR_BGR2RGB))
# axarr[1].title.set_text('Denoised Input')
# axarr[1].axis('off')

#@title Run to trigger inpainting. { display-mode: "form" }
!python inpaint.py

"""## **OUTPUT COMPARISION**"""

#@title Run to check the output.{ display-mode: "form" }



input_image = cv2.imread("input/img.jpg")
input_image_dn = cv2.imread("input/imgs/img.jpg")
input_mask = cv2.imread("input/mask.png")
output_image = cv2.imread("output/imgs/img.jpg")
output_image_sharpned=sharpen_image("output/imgs/img.jpg","output/img.jpg")

f, axarr = plt.subplots(1,4, figsize=(15,15))
axarr[0].imshow(cv2.cvtColor(cv2.resize(input_image, resize_size), cv2.COLOR_BGR2RGB))
axarr[0].title.set_text('Actual Input')
axarr[0].axis('off')

axarr[1].imshow(cv2.cvtColor(cv2.resize(input_image_dn, resize_size), cv2.COLOR_BGR2RGB))
axarr[1].title.set_text('Denoised Input')
axarr[1].axis('off')

axarr[2].imshow(cv2.cvtColor(cv2.resize(input_mask, resize_size), cv2.COLOR_BGR2RGB))
axarr[2].title.set_text('Mask')
axarr[2].axis('off')


axarr[3].imshow(cv2.cvtColor(cv2.resize(output_image, resize_size), cv2.COLOR_BGR2RGB))
axarr[3].title.set_text('Inpainted Output')
axarr[3].axis('off')


# axarr[4].imshow(cv2.cvtColor(cv2.resize(output_image_sharpned, resize_size), cv2.COLOR_BGR2RGB))
# axarr[4].title.set_text('sharpned Inpainted Output')
# axarr[4].axis('off')

"""Upload new images and run the trigger cell to observe outputs on different images."""

#@title Calculate PSNR.{ display-mode: "form" }

from deepfillv2.utils import ssim,psnr
import torch


ev_input_image = cv2.resize(input_image, resize_size)
ev_output_image = cv2.resize(output_image, resize_size)

ev_input_image = torch.from_numpy(ev_input_image)
ev_output_image = torch.from_numpy(ev_output_image)

ev_input_image = ev_input_image.float()
ev_output_image = ev_output_image.float()

ev_input_image = ev_input_image.unsqueeze(0)
ev_output_image = ev_output_image.unsqueeze(0)

ev_input_image = ev_input_image.cpu()
ev_output_image = ev_output_image.cpu()

print(psnr(ev_input_image, ev_output_image))
#ssim(ev_input_image, ev_output_image)

#@title Calculate SSIM.{ display-mode: "form" }
from skimage.metrics import structural_similarity

before = cv2.resize(input_image, resize_size)
after = cv2.resize(output_image, resize_size)


# Convert images to grayscale
before_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY)
after_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY)

# Compute SSIM between two images
(score, diff) = structural_similarity(before_gray, after_gray, full=True)
print("Image similarity", score)

# The diff image contains the actual image differences between the two images
# and is represented as a floating point data type in the range [0,1]
# so we must convert the array to 8-bit unsigned integers in the range
# [0,255] before we can use it with OpenCV
diff = (diff * 255).astype("uint8")

# Threshold the difference image, followed by finding contours to
# obtain the regions of the two input images that differ
thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]
contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
contours = contours[0] if len(contours) == 2 else contours[1]

mask = np.zeros(before.shape, dtype='uint8')
filled_after = after.copy()

!pip install torchmetrics dnnlib-util scipy torchmetrics[image] torch-fidelity

#@title Measures

_ = torch.manual_seed(123)
from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
from torchmetrics.image.fid import FrechetInceptionDistance


#fid = FrechetInceptionDistance(feature=64)
lpips = LearnedPerceptualImagePatchSimilarity(net_type='squeeze')
# LPIPS needs the images to be in the [-1, 1] range.
img1 =  ev_input_image
img2 = ev_output_image
img1_fid =  torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)
img2_fid = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)

normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
img1 = normalize(img1)
img2 = normalize(img2)

lpipsval=lpips(img1, img2)
# fid.update(img1_fid, real=True)
# fid.update(img2_fid, real=False)
# fidval=fid.compute()

print(f"LPIPS= {lpipsval}")
# print(f"FID= {fidval}")